# spider-test

## 使用cheerio解析网页，nodejs爬虫尝试，爬取院系某栏的新闻文本

## 模拟登陆pixiv
### pixiv登录过程抓包观察
1. 利用Fiddler插件观察登录时的form表单字段
2. 获取网页的cookie，参考了**kokororin**的[pixiv-cookie](https://github.com/kokororin/pixiv-cookie)
3. 模拟发送https报文，利用got模块实现登录处理(got底层利用promise、http实现)

> * 2017-04-23完成

## 爬取信息
### 爬取一个用的关注列表
1. 观察url特征，如下所示，id为用户id，p为第几页<code>https://www.pixiv.net/bookmark.php?type=user&id=2327032&p=5</code>
2. 观察网页结构，找到用户列表的关键信息，总关注数目total，一页的最多显示数目MAX_PER_PAGE（48）
3. 爬取第一页，获取total等信息
4. 根据total/MAX_PER_PAGE爬取后续剩余的关注用户

> * 2017-04-24

### 根据用户的关注列表爬取所有的用户列表
1. 从一个种子出发，爬取了2747个用户的关注列表，然后内存炸了。。。炸了。。。
2. 打算把大部分待搜索的id放入文件中，防止内存炸了的情况

> * 2017-04-25完成

3. 目前爬取了60万+条有效id,然后ip被封了。。。

> * 2017-04-27

4. 写完爬取作品详情信息，某用户的作品等
5. 可以考虑用生产者消费者的模型，利用EventEmitter监听器来实现两者间的同步

> * 2017-04-28

### 去重
去重方面使用了布隆过滤器

### 根据所有的用户列表爬取所有的作品信息
利用生产者消费者模式进行编写代码
断线后切换账号重连实现了用户作品的爬取

> * 作品的收藏数还是没有办法爬到，目前测试出来的原因是直接使用cookie和sessionid他会强制跳到登录界面，然后验证你是否已经登录等。


## 入库存储

## 建立网站搜索查找

## 客户端
